{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Algo8_AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBzBiaC9ku-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71zPHmdPBRyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the dataset given for any phone:-accel or gyro / watch:-accel or gyro couldnt be fitted in a single csv file (for all 20 data in excel stacked above each other) , \n",
        "# so I loaded first 14 data set in 1 file and the rest in the 2nd file\n",
        "def load_data():\n",
        "    dataset = pd.read_csv('/content/drive/My Drive/Train2.csv', header=None)\n",
        "    train_set_x_orig1 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig1 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    dataset = pd.read_csv('/content/drive/My Drive/Train2.csv', header=None)\n",
        "    train_set_x_orig2 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig2 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    train_set_x_orig = np.vstack((train_set_x_orig1,train_set_x_orig2)) # stacking the two csv file one over the other in numpy array\n",
        "    train_set_y_orig = np.vstack((train_set_y_orig1,train_set_y_orig2))\n",
        "\n",
        "    train_x = train_set_x_orig.T\n",
        "    train_y = train_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    train_y= onehotencoder.fit_transform(train_y[:,[0]]).toarray()\n",
        "    train_y = train_y.T\n",
        "\n",
        "    dataset = pd.read_csv('/content/drive/My Drive/Test1.csv', header=None)\n",
        "    test_set_x_orig = dataset.iloc[:,[3,4,5]].values # taking the useful train set features\n",
        "    test_set_y_orig = dataset.iloc[:,[1]].values\n",
        "\n",
        "    test_x = test_set_x_orig.T\n",
        "    test_y = test_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    test_y= onehotencoder.fit_transform(test_y[:,[0]]).toarray()\n",
        "    test_y = test_y.T\n",
        "\n",
        "    return train_x,train_y,test_x,test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1QL1E8y-gGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary for reversing onehotencoding at in the predict function at the end\n",
        "dict = {0 :'A' ,\n",
        "        1 :'B' ,\n",
        "        2 :'C' ,\n",
        "        3 :'D' ,\n",
        "        4 :'E' ,\n",
        "        5 :'F' ,\n",
        "        6 :'G' ,\n",
        "        7 :'H' ,\n",
        "        8 :'I' ,\n",
        "        9 :'J' ,\n",
        "        10 :'K' ,\n",
        "        11:'L' ,\n",
        "        12 :'M' ,\n",
        "        13:'O' ,\n",
        "        14:'P' ,\n",
        "        15:'Q' ,\n",
        "        16:'R' ,\n",
        "        17:'S' }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeizjjIeA3Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(Z):\n",
        "\n",
        "    #Z -- numpy array of any shape\n",
        "    \n",
        "    \n",
        "    #A -- output of sigmoid(z), same shape as Z\n",
        "    #cache -- returns Z as well, useful during backpropagation\n",
        "    \n",
        "    x_exp = np.exp(Z)\n",
        "    x_sum = np.sum(x_exp,axis=0,keepdims=True)\n",
        "    A = x_exp/x_sum\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAOgTdkA6zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "  \n",
        "    #Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    #A -- Post-activation parameter, of the same shape as Z\n",
        "    #cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNwX78-TB0cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "\n",
        "    #dA -- post-activation gradient, of any shape\n",
        "    #cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    \n",
        "    #dZ -- Gradient of the cost with respect to Z\n",
        "   \n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, then dz to 0 \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RANI1JzLB0lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_backward(dA, cache):\n",
        "\n",
        "    #dA -- post-activation gradient, of any shape\n",
        "    #cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    \n",
        "    #dZ -- Gradient of the cost with respect to Z\n",
        "    \n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    dZ = dA * 1\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lNsn4dwB0qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "\n",
        "    #layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "\n",
        "    #parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD4vHhUFB0tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "\n",
        "    #A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    #W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    #b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    #Z -- the input of the activation function\n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \n",
        "    Z = W.dot(A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfkinIliB0vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "    #A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    #W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    #b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    #activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    #A -- the output of the activation function, also called the post-activation value \n",
        "    #cache -- a dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             \n",
        "    \n",
        "    if activation == \"softmax\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N78YcQrhB0ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    #X -- data, numpy array of shape (input size, number of examples)\n",
        "    #parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    #AL -- last post-activation value\n",
        "    #caches -- list of caches \n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implementing [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implementing LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (18,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYCLqHg1B00y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "\n",
        "    #AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    #Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "    #cost -- cross-entropy cost\n",
        "\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Loss from aL and y.\n",
        "    cost = (1./m) * np.sum(np.sum(-np.multiply(Y,np.log(AL)),axis=0))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7jEaDEB03n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    #dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    #cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "\n",
        "    #dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    #dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    #db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm0GCI75B06Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    #dA -- post-activation gradient for current layer l \n",
        "    #cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    #activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    #dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    #dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    #db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"softmax\":\n",
        "        dZ = softmax_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DEweF_uB085",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    #AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    #Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    #caches -- list of cache\n",
        "    #grads -- A dictionary with the gradients\n",
        "\n",
        " \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = AL-Y\n",
        "    \n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdrcrfx7B0_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    #parameters -- python dictionary containing your parameters \n",
        "    #grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    #parameters -- python dictionary containing teh updated parameters \n",
        "                \n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Updated rule for each parameter. Using a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZC8DL64NMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y, test_x, test_y= load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtShtr6CiKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##reshape and standardise here\n",
        "# Reshape the training and test examples \n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "sc = StandardScaler()\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n",
        "train_x[:, :] = sc.fit_transform(train_x[:, :])\n",
        "test_x[:, :] = sc.transform(test_x[:, :])\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q4f5W9RCiTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#  We can set the number of nodes in each layer and the total number of layers\n",
        "layers_dims = [3, 30, 25, 25, 18] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeInB5x9CicY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000,print_cost = False):#lr was 0.009\n",
        "\n",
        "\n",
        "    #layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    #learning_rate -- learning rate of the gradient descent update rule\n",
        "    #num_iterations -- number of iterations of the optimization loop\n",
        " \n",
        "    #parameters -- parameters learnt by the model. They can then be used to predict.\n",
        " \n",
        "    np.random.seed(1)\n",
        "    costs = []                         \n",
        "    \n",
        "\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    \n",
        "    # Loop for gradient descent\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "\n",
        "\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTLttfoCilK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model train\n",
        "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 200, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PapyLW1CB1Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#accuracy prediction and result\n",
        "def predict(X, y, parameters):\n",
        "\n",
        "    #p -- predictions for the given dataset re - converted to the original way after doing onehotencoding using the help of dictionary made at the beginning\n",
        " \n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "      new_list = list(probas[:,i])\n",
        "      maxm = new_list.index(max(new_list))\n",
        "      probas[maxm,i]=1\n",
        "    probas = (probas>=1).astype(int)\n",
        "    \n",
        "    prob = probas.T\n",
        "    y = y.T\n",
        "    incorrect = np.sum(y!=prob)/2\n",
        "    accu = (m-incorrect)/m\n",
        "\n",
        "    print(\"Accuracy: \"  + str(accu))\n",
        "\n",
        "    #reversing onehotencoding with help of dictionary\n",
        "    nrow = y.shape[0]\n",
        "    n= np.argmax(y, axis = 0)\n",
        "    n_list = list(n)\n",
        "    n_final = []\n",
        "    for i in range(nrow):\n",
        "     indi = n_list[i]\n",
        "     n_dic = dict[indi]\n",
        "     n_final.append(n_dic)\n",
        "    n_final = np.array(n_final)\n",
        "    new = np.reshape(n_final,(nrow,1))\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ickzn1hlDoMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train = predict(train_x, train_y, parameters) #accuracy prediction"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHcqy3PBDoOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test = predict(test_x, test_y, parameters) #accuracy prediction"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}