{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algo8_AI()4 layer neural network with minibatch and adam).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBzBiaC9ku-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71zPHmdPBRyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the dataset given for any phone:-accel or gyro / watch:-accel or gyro couldnt be fitted in a single csv file (for all 20 data in excel stacked above each other) , \n",
        "# so I loaded first 14 data set in 1 file and the rest in the 2nd file\n",
        "def load_data():\n",
        "    dataset = pd.read_csv('Train1 FILE HERE.csv', header=None)\n",
        "    train_set_x_orig1 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig1 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    dataset = pd.read_csv('Train2 FILE HERE.csv', header=None)\n",
        "    train_set_x_orig2 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig2 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    train_set_x_orig = np.vstack((train_set_x_orig1,train_set_x_orig2)) # stacking the two csv file one over the other in numpy array\n",
        "    train_set_y_orig = np.vstack((train_set_y_orig1,train_set_y_orig2))\n",
        "\n",
        "    train_x = train_set_x_orig.T\n",
        "    train_y = train_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    train_y= onehotencoder.fit_transform(train_y[:,[0]]).toarray()\n",
        "    train_y = train_y.T\n",
        "\n",
        "    dataset = pd.read_csv('Test1 FILE HERE.csv', header=None)\n",
        "    test_set_x_orig1 = dataset.iloc[:,[3,4,5]].values # taking the useful train set features\n",
        "    test_set_y_orig1 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    dataset = pd.read_csv('Test1 FILE HERE.csv', header=None)\n",
        "    test_set_x_orig2 = dataset.iloc[:,[3,4,5]].values # taking the useful train set features\n",
        "    test_set_y_orig2 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    test_set_x_orig = np.vstack((test_set_x_orig1,test_set_x_orig2)) # stacking the two csv file one over the other in numpy array\n",
        "    test_set_y_orig = np.vstack((test_set_y_orig1,test_set_y_orig2))\n",
        "\n",
        "    test_x = test_set_x_orig.T\n",
        "    test_y = test_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    test_y= onehotencoder.fit_transform(test_y[:,[0]]).toarray()\n",
        "    test_y = test_y.T\n",
        "\n",
        "    return train_x,train_y,test_x,test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1QL1E8y-gGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dict = {0 :'A' ,\n",
        "        1 :'B' ,\n",
        "        2 :'C' ,\n",
        "        3 :'D' ,\n",
        "        4 :'E' ,\n",
        "        5 :'F' ,\n",
        "        6 :'G' ,\n",
        "        7 :'H' ,\n",
        "        8 :'I' ,\n",
        "        9 :'J' ,\n",
        "        10 :'K' ,\n",
        "        11:'L' ,\n",
        "        12 :'M' ,\n",
        "        13:'O' ,\n",
        "        14:'P' ,\n",
        "        15:'Q' ,\n",
        "        16:'R' ,\n",
        "        17:'S' }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeizjjIeA3Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(X):\n",
        "    \n",
        "    #A -- output of sigmoid(z), same shape as Z\n",
        "\n",
        "    \n",
        "    x_exp = np.exp(X)\n",
        "    x_sum = np.sum(x_exp,axis=0,keepdims=True)\n",
        "    A = x_exp/x_sum\n",
        "    \n",
        "    return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAOgTdkA6zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(X):\n",
        "  \n",
        "\n",
        "    #A -- Post-activation parameter, of the same shape as Z\n",
        "\n",
        "    \n",
        "    A = np.maximum(0,X)\n",
        "    \n",
        "    return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lNsn4dwB0qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "\n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims) # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*  np.sqrt(2 / layer_dims[l-1])\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert parameters['W' + str(l)].shape[0] == layer_dims[l], layer_dims[l-1]\n",
        "        assert parameters['W' + str(l)].shape[0] == layer_dims[l], 1\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecPWLg7tjYVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(a4, Y):\n",
        "\n",
        "    logprobs = np.multiply(-np.log(a4),Y) + np.multiply(-np.log(1 - a4), 1 - Y)\n",
        "    cost_total =  np.sum(logprobs)\n",
        "    \n",
        "    return cost_total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8pnUr8ckLhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        " \n",
        "    # retrieve parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "    W3 = parameters[\"W3\"]\n",
        "    b3 = parameters[\"b3\"]\n",
        "    W4 = parameters[\"W4\"]\n",
        "    b4 = parameters[\"b4\"]\n",
        "    \n",
        "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
        "    z1 = np.dot(W1, X) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(W2, a1) + b2\n",
        "    a2 = relu(z2)\n",
        "    z3 = np.dot(W3, a2) + b3\n",
        "    a3 = relu(z3)\n",
        "    z4 = np.dot(W4, a3) + b4\n",
        "    a4 = softmax(z4)\n",
        "    \n",
        "    cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3, z4, a4, W4, b4)\n",
        "    \n",
        "    return a4, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUr6Q2_Vk7dX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_propagation(X, Y, cache):\n",
        "\n",
        "    m = X.shape[1]\n",
        "    (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3, z4, a4, W4, b4) = cache\n",
        "    \n",
        "    dz4 = 1./m * (a4 - Y)\n",
        "    dW4 = np.dot(dz4, a3.T)\n",
        "    db4 = np.sum(dz4, axis=1, keepdims = True)\n",
        "    \n",
        "    da3 = np.dot(W4.T, dz4)\n",
        "    dz3 = np.multiply(da3, np.int64(a3 > 0))\n",
        "    dW3 = np.dot(dz3, a1.T)\n",
        "    db3 = np.sum(dz3, axis=1, keepdims = True)\n",
        "    \n",
        "    da2 = np.dot(W3.T, dz3)\n",
        "    dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
        "    dW2 = np.dot(dz2, a1.T)\n",
        "    db2 = np.sum(dz2, axis=1, keepdims = True)\n",
        "    \n",
        "    da1 = np.dot(W2.T, dz2)\n",
        "    dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
        "    dW1 = np.dot(dz1, X.T)\n",
        "    db1 = np.sum(dz1, axis=1, keepdims = True)\n",
        "    \n",
        "    gradients = {\"dz4\": dz4, \"dW4\": dW4, \"db4\": db4,\n",
        "                 \"dz3\": dz3, \"dW3\": dW3, \"db3\": db3, \"db3\": db3,\n",
        "                 \"da2\": da2, \"dz2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"da1\": da1, \"dz1\": dz1, \"dW1\": dW1, \"db1\": db1}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49NTw6jnlBHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(X, y, parameters):\n",
        "\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    p = np.zeros((1,m), dtype = np.int)\n",
        "    \n",
        "    # Forward propagation\n",
        "    a4, caches = forward_propagation(X, parameters)\n",
        "    \n",
        "\n",
        "    for i in range(0, a4.shape[1]):\n",
        "      new_list = list(a4[:,i])\n",
        "      maxm = new_list.index(max(new_list))\n",
        "      a4[maxm,i]=1\n",
        "    a4 = (a4>=1).astype(int)\n",
        "    \n",
        "    prob = a4.T\n",
        "    y = y.T\n",
        "    incorrect = np.sum(y!=prob)/2\n",
        "    accu = (m-incorrect)/m\n",
        "\n",
        "    print(\"Accuracy: \"  +  str(accu))\n",
        "    \n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El5PkA8FnHyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "\n",
        "    \n",
        "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
        "    m = X.shape[1]                  # number of training examples\n",
        "    mini_batches = []\n",
        "        \n",
        "\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((18,m))\n",
        "\n",
        "\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, num_complete_minibatches):\n",
        "\n",
        "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "\n",
        "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n",
        "\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_LEo-iSn8BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_adam(parameters) :\n",
        "\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "\n",
        "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrIAb7Qoj15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
        "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        " \n",
        "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "\n",
        "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "\n",
        "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
        "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
        "\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "\n",
        "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
        "\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZC8DL64NMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y, test_x, test_y= load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtShtr6CiKy",
        "colab_type": "code",
        "outputId": "b03664b4-1438-406e-ba47-cccb575bdc4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#standardise here\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n",
        "train_x[:, :] = sc.fit_transform(train_x[:, :])\n",
        "test_x[:, :] = sc.transform(test_x[:, :])\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n",
        "print (\"train_x's shape: \" + str(train_x.shape))\n",
        "print (\"test_x's shape: \" + str(test_x.shape))\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (3, 1580245)\n",
            "test_x's shape: (3, 111026)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeInB5x9CicY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(X, Y, layers_dims, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
        "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):\n",
        "\n",
        "    L = len(layers_dims)             # number of layers in the neural networks\n",
        "    costs = []                       # to keep track of the cost\n",
        "    t = 0                            # initializing the counter required for Adam update\n",
        "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
        "    m = X.shape[1]                   # number of training examples\n",
        "    \n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    v, s = initialize_adam(parameters)\n",
        "    \n",
        "    # Optimization loop\n",
        "    for i in range(num_epochs):\n",
        "        \n",
        "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
        "        seed = seed + 1\n",
        "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
        "        cost_total = 0\n",
        "        \n",
        "        for minibatch in minibatches:\n",
        "\n",
        "            # Select a minibatch\n",
        "            (minibatch_X, minibatch_Y) = minibatch\n",
        "\n",
        "            # Forward propagation\n",
        "            a4, caches = forward_propagation(minibatch_X, parameters)\n",
        "\n",
        "            # Compute cost and add to the cost total\n",
        "            cost_total += compute_cost(a4, minibatch_Y)\n",
        "\n",
        "            # Backward propagation\n",
        "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
        "\n",
        "            t = t + 1 # Adam counter\n",
        "            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n",
        "        \n",
        "        cost_avg = cost_total / m\n",
        "        \n",
        "        # Print the cost every 1000 epoch\n",
        "        if print_cost and i % 1000 == 0:\n",
        "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost_avg)\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTLttfoCilK",
        "colab_type": "code",
        "outputId": "e245a246-a3b8-4c05-ce37-929b795ecab9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        }
      },
      "source": [
        "#model train\n",
        "layers_dims = [3, 30, 25, 25, 18] #  4-layer model\n",
        "parameters = model(train_x, train_y, layers_dims, num_epochs = 200, print_cost = True)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-109eaeb16069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m18\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#  4-layer model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-20e882042b06>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X, Y, layers_dims, learning_rate, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Adam counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters_with_adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mcost_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcost_total\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-4a68233dbf7c>\u001b[0m in \u001b[0;36mupdate_parameters_with_adam\u001b[0;34m(parameters, grads, v, s, t, learning_rate, beta1, beta2, epsilon)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m### START CODE HERE ### (approx. 2 lines)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dW\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"db\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (25,25) (25,30) "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ickzn1hlDoMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train = predict(train_x, train_y, parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHcqy3PBDoOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_test = predict(test_x, test_y, parameters)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}