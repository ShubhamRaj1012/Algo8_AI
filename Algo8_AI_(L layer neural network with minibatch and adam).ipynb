{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Algo8_AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGBzBiaC9ku-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71zPHmdPBRyC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the dataset given for any phone:-accel or gyro / watch:-accel or gyro couldnt be fitted in a single csv file (for all 20 data in excel stacked above each other) , \n",
        "# so I loaded first 14 data set in 1 file and the rest in the 2nd file\n",
        "def load_data():\n",
        "    dataset = pd.read_csv('/content/drive/My Drive/Train1.csv', header=None)\n",
        "    train_set_x_orig1 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig1 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    dataset = pd.read_csv('/content/drive/My Drive/Train2.csv', header=None)\n",
        "    train_set_x_orig2 = dataset.iloc[:,[3,4,5]].values # your train set features\n",
        "    train_set_y_orig2 = dataset.iloc[:,[1]].values\n",
        "\n",
        "    train_set_x_orig = np.vstack((train_set_x_orig1,train_set_x_orig2)) # stacking the two csv file one over the other in numpy array\n",
        "    train_set_y_orig = np.vstack((train_set_y_orig1,train_set_y_orig2))\n",
        "\n",
        "    train_x = train_set_x_orig.T\n",
        "    train_y = train_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    train_y= onehotencoder.fit_transform(train_y[:,[0]]).toarray()\n",
        "    train_y = train_y.T\n",
        "\n",
        "    dataset = pd.read_csv('Test1.csv', header=None)\n",
        "    test_set_x_orig = dataset.iloc[:,[3,4,5]].values # taking the useful train set features\n",
        "    test_set_y_orig = dataset.iloc[:,[1]].values\n",
        "\n",
        "    test_x = test_set_x_orig.T\n",
        "    test_y = test_set_y_orig\n",
        "    onehotencoder=OneHotEncoder()\n",
        "    test_y= onehotencoder.fit_transform(test_y[:,[0]]).toarray()\n",
        "    test_y = test_y.T\n",
        "\n",
        "    return train_x,train_y,test_x,test_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1QL1E8y-gGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary for reversing onehotencoding at in the predict function at the end\n",
        "dict = {0 :'A' ,\n",
        "        1 :'B' ,\n",
        "        2 :'C' ,\n",
        "        3 :'D' ,\n",
        "        4 :'E' ,\n",
        "        5 :'F' ,\n",
        "        6 :'G' ,\n",
        "        7 :'H' ,\n",
        "        8 :'I' ,\n",
        "        9 :'J' ,\n",
        "        10 :'K' ,\n",
        "        11:'L' ,\n",
        "        12 :'M' ,\n",
        "        13:'O' ,\n",
        "        14:'P' ,\n",
        "        15:'Q' ,\n",
        "        16:'R' ,\n",
        "        17:'S' }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeizjjIeA3Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(Z):\n",
        "\n",
        "    #Z -- numpy array of any shape\n",
        "    \n",
        "    \n",
        "    #A -- output of sigmoid(z), same shape as Z\n",
        "    #cache -- returns Z as well, useful during backpropagation\n",
        "    \n",
        "    x_exp = np.exp(Z)\n",
        "    x_sum = np.sum(x_exp,axis=0,keepdims=True)\n",
        "    A = x_exp/x_sum\n",
        "    cache = Z\n",
        "    \n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlAOgTdkA6zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(Z):\n",
        "  \n",
        "    #Z -- Output of the linear layer, of any shape\n",
        "\n",
        "    #A -- Post-activation parameter, of the same shape as Z\n",
        "    #cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
        "    \n",
        "    A = np.maximum(0,Z)\n",
        "    \n",
        "    assert(A.shape == Z.shape)\n",
        "    \n",
        "    cache = Z \n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNwX78-TB0cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "\n",
        "    #dA -- post-activation gradient, of any shape\n",
        "    #cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    \n",
        "    #dZ -- Gradient of the cost with respect to Z\n",
        "   \n",
        "    \n",
        "    Z = cache\n",
        "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
        "    \n",
        "    # When z <= 0, then dz to 0 \n",
        "    dZ[Z <= 0] = 0\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RANI1JzLB0lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax_backward(dA, cache):\n",
        "\n",
        "    #dA -- post-activation gradient, of any shape\n",
        "    #cache -- 'Z' where we store for computing backward propagation efficiently\n",
        "\n",
        "    \n",
        "    #dZ -- Gradient of the cost with respect to Z\n",
        "    \n",
        "    \n",
        "    Z = cache\n",
        "    \n",
        "    dZ = dA * 1\n",
        "    \n",
        "    assert (dZ.shape == Z.shape)\n",
        "    \n",
        "    return dZ"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lNsn4dwB0qq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "\n",
        "    #layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "\n",
        "    #parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "    #Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "    #bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \n",
        "    np.random.seed(1)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1])*0.01\n",
        "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iD4vHhUFB0tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "\n",
        "    #A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    #W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    #b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    #Z -- the input of the activation function\n",
        "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \n",
        "    Z = W.dot(A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfkinIliB0vo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "\n",
        "    #A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    #W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    #b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    #activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    #A -- the output of the activation function, also called the post-activation value \n",
        "    #cache -- a dictionary containing \"linear_cache\" and \"activation_cache\";\n",
        "             \n",
        "    \n",
        "    if activation == \"softmax\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = softmax(Z)\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = relu(Z)\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N78YcQrhB0ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "\n",
        "    #X -- data, numpy array of shape (input size, number of examples)\n",
        "    #parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    #AL -- last post-activation value\n",
        "    #caches -- list of caches \n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    # Implementing [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    # Implementing LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
        "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"softmax\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (18,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYCLqHg1B00y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "\n",
        "    #AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    #Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "    #cost -- cross-entropy cost\n",
        "\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Loss from aL and y.\n",
        "    cost = (1./m) * np.sum(np.sum(-np.multiply(Y,np.log(AL)),axis=0))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect \n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I7jEaDEB03n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "\n",
        "    #dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    #cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "\n",
        "    #dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    #dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    #db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
        "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm0GCI75B06Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "\n",
        "    #dA -- post-activation gradient for current layer l \n",
        "    #cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    #activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    #dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    #dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    #db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \n",
        "    linear_cache, activation_cache = cache\n",
        "    \n",
        "    if activation == \"relu\":\n",
        "        dZ = relu_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"softmax\":\n",
        "        dZ = softmax_backward(dA, activation_cache)\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DEweF_uB085",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "\n",
        "    #AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    #Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    #caches -- list of cache\n",
        "    #grads -- A dictionary with the gradients\n",
        "\n",
        " \n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = AL-Y\n",
        "    \n",
        "    \n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"softmax\")\n",
        "    \n",
        "    for l in reversed(range(L-1)):\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdrcrfx7B0_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "\n",
        "    #parameters -- python dictionary containing your parameters \n",
        "    #grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    #parameters -- python dictionary containing teh updated parameters \n",
        "                \n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "    # Updated rule for each parameter. Using a for loop.\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfZC8DL64NMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_x, train_y, test_x, test_y= load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFtShtr6CiKy",
        "colab_type": "code",
        "outputId": "df007353-de58-4438-bb67-f3e62eadb769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "##reshape and standardise here\n",
        "# Reshape the training and test examples \n",
        "\n",
        "# Standardize data to have feature values between 0 and 1.\n",
        "sc = StandardScaler()\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n",
        "train_x[:, :] = sc.fit_transform(train_x[:, :])\n",
        "test_x[:, :] = sc.transform(test_x[:, :])\n",
        "\n",
        "train_x =train_x.T\n",
        "test_x = test_x.T\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_x's shape: (3, 1580245)\n",
            "test_x's shape: (3, 111026)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "El5PkA8FnHyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
        "   \n",
        "    #Createing a list of random minibatches from (X, Y)\n",
        "    \n",
        " \n",
        "    #X -- input data, of shape (input size, number of examples)\n",
        "    #Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
        "    #mini_batch_size -- size of the mini-batches, integer\n",
        "    #mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
        "  \n",
        "    \n",
        "    np.random.seed(seed)           \n",
        "    m = X.shape[1]                 \n",
        "    mini_batches = []\n",
        "        \n",
        "    # Shuffling (X, Y)\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[:, permutation]\n",
        "    shuffled_Y = Y[:, permutation].reshape((18,m))\n",
        "\n",
        "    # Partition Minus the end case.\n",
        "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size \n",
        "    for k in range(0, num_complete_minibatches):\n",
        "      \n",
        "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
        " \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    # Handling the end case (last mini-batch < mini_batch_size)\n",
        "    if m % mini_batch_size != 0:\n",
        "\n",
        "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
        "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n",
        "\n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_LEo-iSn8BG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_adam(parameters) :\n",
        "\n",
        "    #parameters -- python dictionary containing parameters.\n",
        "    #v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        " \n",
        "    #s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "\n",
        "\n",
        "   \n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initializing v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "\n",
        "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\n",
        "        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\n",
        "\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrIAb7Qoj15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\n",
        "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "   \n",
        "    #Updating parameters using Adam\n",
        "    \n",
        " \n",
        "    #parameters -- python dictionary \n",
        "    #grads -- python dictionary containing gradients \n",
        "    #v -- Adam variable\n",
        "    #s -- Adam variable\n",
        "    #learning_rate -- the learning rate, scalar.\n",
        "    #beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    #beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    #epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         \n",
        "    s_corrected = {}                         \n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "  \n",
        "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "\n",
        "\n",
        "        # Computing bias-corrected first moment estimate\n",
        "       \n",
        "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
        "        \n",
        "\n",
        "        # Moving average of the squared gradients.\n",
        "     \n",
        "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
        "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
        "        \n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. \n",
        "     \n",
        "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "       \n",
        "\n",
        "        # Updating parameters\n",
        "\n",
        "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
        "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
        "\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeInB5x9CicY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, mini_batch_size=1024, beta = 0.9,beta1= 0.9,beta2=0.999,epsilon=1e-8, num_epochs = 3000,print_cost = False):\n",
        "   \n",
        "\n",
        "    #layers_dims -- list containing the input size and each layer size, of length \n",
        "    #learning_rate -- learning rate of the gradient descent update rule\n",
        "    #num_iterations -- number of iterations of the optimization loop\n",
        "    \n",
        "\n",
        "    #parameters -- parameters learnt by the model.\n",
        "    \n",
        "\n",
        "    seed = 10\n",
        "    costs = []                         \n",
        "    t = 0\n",
        "    \n",
        " \n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "\n",
        "    v,s = initialize_adam(parameters)\n",
        "    \n",
        "    # Loop of gradient descent\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        seed=seed+1\n",
        "        minibatches = random_mini_batches(X,Y, mini_batch_size,seed)\n",
        "        for minibatch in minibatches:\n",
        "\n",
        "          (minibatch_X,minibatch_Y) = minibatch\n",
        "\n",
        "\n",
        "          AL, caches = L_model_forward(minibatch_X, parameters)\n",
        "\n",
        "          cost = compute_cost(AL, minibatch_Y)\n",
        "\n",
        "          grads = L_model_backward(AL, minibatch_Y, caches)\n",
        "\n",
        "          parameters,v,s = update_parameters_with_adam(parameters, grads,v,s,t,learning_rate,beta1,beta2,epsilon)\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vTLttfoCilK",
        "colab_type": "code",
        "outputId": "d12bd728-dea3-4e40-aa94-86c03ffc7af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "#model train\n",
        "layers_dims = [3, 30, 25, 25, 18] #  We can set the number of nodes in each layer and the total number of layers\n",
        "parameters = L_layer_model(train_x, train_y, layers_dims, num_epochs = 200, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: divide by zero encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:58: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:52: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: invalid value encountered in true_divide\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in less_equal\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after epoch 0: nan\n",
            "Cost after epoch 100: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PapyLW1CB1Cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#accuracy prediction and result\n",
        "def predict(X, y, parameters):\n",
        "\n",
        "    #p -- predictions for the given dataset re - converted to the original way after doing onehotencoding using the help of dictionary made at the beginning\n",
        " \n",
        "    \n",
        "    m = X.shape[1]\n",
        "    n = len(parameters) // 2 # number of layers in the neural network\n",
        "    p = np.zeros((1,m))\n",
        "    \n",
        "    # Forward propagation\n",
        "    probas, caches = L_model_forward(X, parameters)\n",
        "\n",
        "    \n",
        "    # convert probas to 0/1 predictions\n",
        "    for i in range(0, probas.shape[1]):\n",
        "      new_list = list(probas[:,i])\n",
        "      maxm = new_list.index(max(new_list))\n",
        "      probas[maxm,i]=1\n",
        "    probas = (probas>=1).astype(int)\n",
        "    \n",
        "    prob = probas.T\n",
        "    y = y.T\n",
        "    incorrect = np.sum(y!=prob)/2\n",
        "    accu = (m-incorrect)/m\n",
        "\n",
        "    print(\"Accuracy: \"  + str(accu))\n",
        "\n",
        "    #reversing onehotencoding with help of dictionary\n",
        "    nrow = y.shape[0]\n",
        "    n= np.argmax(y, axis = 0)\n",
        "    n_list = list(n)\n",
        "    n_final = []\n",
        "    for i in range(nrow):\n",
        "     indi = n_list[i]\n",
        "     n_dic = dict[indi]\n",
        "     n_final.append(n_dic)\n",
        "    n_final = np.array(n_final)\n",
        "    new = np.reshape(n_final,(nrow,1))\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ickzn1hlDoMQ",
        "colab_type": "code",
        "outputId": "550735f3-69bf-4920-ff94-9a0bef94b931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "pred_train = predict(train_x, train_y, parameters) #accuracy prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.058655461653098095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in greater_equal\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHcqy3PBDoOx",
        "colab_type": "code",
        "outputId": "7190f818-e265-4da1-d92c-8096e9da88c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred_test = predict(test_x, test_y, parameters)  #accuracu prediction"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.032217678741916306\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in greater_equal\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}